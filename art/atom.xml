<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: art | Joseph Wilk]]></title>
  <link href="http://blog.josephwilk.net//art/atom.xml" rel="self"/>
  <link href="http://blog.josephwilk.net/"/>
  <updated>2015-08-24T21:45:53+02:00</updated>
  <id>http://blog.josephwilk.net/</id>
  <author>
    <name><![CDATA[Joseph Wilk]]></name>
    <email><![CDATA[joe@josephwilk.net]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Live Coding - Repl Electric]]></title>
    <link href="http://blog.josephwilk.net/art/live-coding-repl-electric.html"/>
    <updated>2014-06-13T16:58:12+02:00</updated>
    <id>http://blog.josephwilk.net/art/live-coding-repl-electric</id>
    <content type="html"><![CDATA[<p>Live coding is the act of turning a programming session into a performance. This can constitute improvisation, music, visuals, poetry, hardware, robots, dance, textiles and people. Pretty much anything with an input and output can be controlled live by programming.</p>

<p>This is not just a performance by programmers for programmers. While this is often where it starts as a live coder, the type of audience and the accessibility of the performance lies in the performers imagination. Abstraction can get us pretty much anywhere.</p>

<p><code>clojure
(def the-stars (dark-matter))
</code></p>

<h2>Repl Electric</h2>

<p><img src="https://camo.githubusercontent.com/d1711cc92a1b79af187344f461be35e2ced44e3f/687474703a2f2f7333302e706f7374696d672e6f72672f7633336377783668642f53637265656e5f53686f745f323031345f30345f32385f61745f32305f31345f33352e706e67"/></p>

<p><a href="http://www.repl-electric.com">Repl Electric</a> is a project I started in order to discover more about music composition and Artificial intelligent based aids to creativity. Which in turn through the inspiration of people like <a href="http://meta-ex.com/">Meta-ex</a> lead me to live programming music.</p>

<p>Here is a performance live coding music and graphics, inspired by a performance in London:</p>

<h3>The Stars</h3>

<iframe src="http://player.vimeo.com/video/95988263" width="500" height="313" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>


<h3>Open Live Coding</h3>

<p>All the tools and code used to create this performance are open for all to see on Github: <a href="https://github.com/repl-electric">https://github.com/repl-electric</a></p>

<p>Three programming languages were used to create this piece:</p>

<ul>
<li>Clojure (Sound)</li>
<li>GLSL (Visuals)</li>
<li>Emacs Lisp (Animations &amp; Navigation)</li>
</ul>


<h3>Tools</h3>

<p>Here are the tools used and a little detail around how they where used in performing &ldquo;The Stars&rdquo;:</p>

<h4>Clojure: <a href="http://clojure.org">http://clojure.org</a></h4>

<p>Clojure is a LISP language based on the JVM.</p>

<p>Clojure focuses on interactive REPL (Read, Evaluate, Print &amp; Loop) driven development. Which makes it a good choice for interactively coding music. It also turns out functional programming is a good fit for operating over music as data.</p>

<h4>Emacs Live: <a href="https://github.com/overtone/emacs-live">https://github.com/overtone/emacs-live</a></h4>

<p>Emacs live is a Emacs release with packages and defaults that are Live Coding centric. Something I use for both for my work and for my live coding.</p>

<p>To execute our code, we launch a repl instance in our project (NEVER launch inside emacs, since then if emacs crashes the repl and music dies) and connect to it from emacs using <code>cider</code> <a href="https://github.com/clojure-emacs/cider.">https://github.com/clojure-emacs/cider.</a></p>

<p>A simple trick to combine Emacs code and visualizations is to launch an OpenGL window in full screen (see Shadertone) and then put a full screen transparent terminal window running emacs over it.</p>

<p><img src="/images/terminal.png" alt="" /></p>

<p>The tumbling text effect seen at the end of the performance is an emacs animation using <code>Zone Mode</code> which supports writing your own text destructors: <a href="http://www.emacswiki.org/emacs/ZoneMode">http://www.emacswiki.org/emacs/ZoneMode</a></p>

<h4>Overtone: <a href="https://github.com/overtone/overtone">https://github.com/overtone/overtone</a></h4>

<p>Overtone is a Clojure based client to <a href="http://supercollider.sourceforge.net">SuperCollider</a>. Supercollider is an environment for real time audio synthesis and algorithmic composition.</p>

<p>Overtone provides us with:</p>

<ul>
<li>Timing (beat generation &ndash; <a href="https://github.com/repl-electric/cassiopeia/blob/master/src/cassiopeia/engine/timing.clj">example timing code</a>).</li>
<li>Building Synths (engineer sound).</li>
<li>Running samples (both your own and from <a href="https://www.freesound.org/">Freesound</a>).</li>
<li>Live Synth control (changing notes, durations, reverb, etc).</li>
<li>Hardware interaction (through midi or OSC).</li>
</ul>


<p>An example of a synth used in The Stars:</p>

<p>```clojure
(use &lsquo;overtone.live)
(defsynth dark-ambience [out-bus 0 amp 1 mul 0.2 room-size 70 rev-time 99 freq 60 ring-mul 55]
  (let [pink (hpf:ar (<em> (</em> 0.005 (pink-noise)) (line:kr 0 1 9)) 5)</p>

<pre><code>    src1 (ringz (* pink (lf-noise1:kr 0.15)) (+ freq (* ring-mul 0)) mul)
    src2 (ringz (* pink (lf-noise1:kr 0.15)) (+ freq (* ring-mul 1)) mul)
    src3 (ringz (* pink (lf-noise1:kr 0.15)) (+ freq (* ring-mul 2)) mul)
    src (tanh (g-verb (sum [src1 src2 src3]) room-size rev-time))]
(out out-bus (* amp src))))
</code></pre>

<p>(def the-stars (dark-ambience))
```</p>

<h6>Timing</h6>

<p>Timing is a complicated issue but so important its worth touching on. You have a choice with Overtone to use Java for timing or Supercollider. I use Supercollider since I have found it to be much more reliable.
Everything you need is here (<a href="https://github.com/repl-electric/cassiopeia/blob/master/src/cassiopeia/engine/timing.clj">copy and paste</a>), thanks to the hard work of <a href="https://github.com/samaaron">Sam Aaron</a>.</p>

<p>The key concept to take away is there are two types of timing, a beat counter which is forever incrementing and a beat trigger which flips back and forth between 1/0.</p>

<p>```clojure
(require &lsquo;[cassiopeia.engine.timing :as time])</p>

<p>;;The beat trigger
(:beat time/main-beat) ;=> 0,1,0,1,0,1,0</p>

<p>;;The beat counter
(:count time/main-beat) ;=> 0,1,2,3,4,5,6,7,8,9
```</p>

<p>The counter is useful for indexing buffers, the trigger is useful in controlling the gate of an envelope (which turns a sound on or off).</p>

<p>In Clojure we can still get access to the beat, in our timing code we send a message using <code>send-trig</code> on every beat. We can hook a Clojure function to callback on this beat:</p>

<p>```
(on-trigger (:trig-id time/main-beat)
  (fn [beat-no]</p>

<pre><code>(when (= 0.0 (mod beat-no 32))
  ;;play a sample
  (boom-s)))
</code></pre>

<p>  ::on-beat-trigger)
```</p>

<p>I use this extensively to time graphic transitions with the music.</p>

<h6>Buffers</h6>

<p>Most of my live coding performance was writing to buffers which are hooked into synths. Buffers are just fixed size arrays but they are stored in Supercollider rather than in Clojure. Here is an example from The Stars where the midi notes are read from a buffer at a rate based on my beat timing signal (a 16th of the main beat here).</p>

<p>```clojure
(use &lsquo;overtone.live)
(use 'cassiopeia.engine.core)
(require &rsquo;[cassiopeia.engine.timing :as time])</p>

<p>(defsynth growl [out-bus 0 note-buf 0 beat-bus 0 beat-trg-bus 0 amp 1]
  (let [cnt (in:kr beat-bus)</p>

<pre><code>    trg (in:kr beat-trg-bus)
    note (buf-rd:kr 1 note-buf cnt)
    freq (midicps note)
    vol (&gt; note 0)

    e (env-gen (perc :attack 10 :sustain 1 :release 1) :gate trg)
    src (lpf (mix [(saw (* 0.25 freq))
                   (sin-osc (* 1.01 freq))]))
    src (pitch-shift src 0.4 1 0 0.01)
    src (pan2:ar (* vol amp e src))]
(out out-bus src)))
</code></pre>

<p>(defonce nebular (buffer 96))</p>

<p>(def nebula (growl :note-buf nebula-note-buf :beat-trg-bus (:beat time/beat-16th) :beat-bus (:count time/beat-16th)))</p>

<p>(pattern! nebula-note-buf (degrees [1 3 7] :major :A2))
```</p>

<h4>GLSL + Shadertone:  <a href="https://github.com/overtone/shadertone">https://github.com/overtone/shadertone</a></h4>

<p>Shaders generate imagery directly on your Graphics Processing Unit rather than going through your CPU. Through a language called GLSL (which is C like) we can express very simple functions which get called on every single pixel generating complex visuals. Here is a simple extract from The Stars that generates all the background small dots:</p>

<p><img src="/images/glsl-dots.png" alt="" /></p>

<p>```glsl
void main(void){
  vec2 current_pixel_position = mod(gl_FragCoord.xy, vec2(5.0)) &ndash; vec2(0.0);
  float distance_squared = dot(current_pixel_position, current_pixel_position);</p>

<p>  vec4 black = vec4(.0, .0, .0, 0.0);
  vec4 white = vec4(1.0, 1.0, 1.0, 1.0);</p>

<p>  //Test the current pixel position and if it should be a circle shade it.
  vec4 circles = (distance_squared &lt; 0.6) ? white : black;</p>

<p>  gl_FragColor = circles;
}
```</p>

<p>For more examples of whats possible with Shaders checkout <a href="https://www.shadertoy.com">Shader Toy</a></p>

<p>Shadertone is the Clojure library that provides a convenient way of running shaders from Clojure and for feeding in data about our synths. It provides access in your Shader to:</p>

<ul>
<li>Overtone&rsquo;s Volume (<code>iOvertoneVolume</code>)</li>
<li>The frequency spectrum &amp; audio waveform data (Passed as a 2D texture <code>:textures [:overtone-audio]</code>)</li>
</ul>


<p>To synchronize the graphics with the music I created a special Overtone synth which does not generate any sound, it instead feeds information in realtime to my shader.</p>

<p>```clojure
(use &lsquo;overtone.live)
(require &rsquo;[shadertone.core :as t])</p>

<p>;;A synth that exposes through taps all the lovely timing information.
(defsynth buffer->tap [beat-buf 0 beat-bus 0 beat-size 16 measure 6]
  (let [cnt (in:kr beat-bus)</p>

<pre><code>    beat (buf-rd:kr 1 beat-buf cnt)
    _  (tap "beat"          60 (a2k beat))
    _  (tap "beat-count"    60 (a2k (mod cnt beat-size)))
    _  (tap "measure-count" 60 (a2k (/ (mod cnt (* measure beat-size)) measure)))])
</code></pre>

<p>  (out 0 0))</p>

<p>;;; Used to store our drum beat, 1 for a hit 0 and for a miss
(defonce drum-sequence-buffer (buffer 256))</p>

<p>(def beats (buffer->tap drum-sequence-buffer (:count timing/main-beat)))</p>

<p>;;Open a OpenGL window running our shader
(t/start-fullscreen &ldquo;resources/shaders/electric.glsl&rdquo;</p>

<pre><code>                :user-data {
                "iBeat"         (atom {:synth beats :tap "beat"})
                "iBeatCount"    (atom {:synth beats :tap "beat-count"})
                "iMeasureCount" (atom {:synth beats :tap "measure-count"})})
</code></pre>

<p>```</p>

<p>Inside our shader code:</p>

<p><code>glsl
uniform float iBeat;
uniform float iBeatCount;
uniform float iMeasureCount;
...
</code></p>

<p>The other main way of controlling a shader from Clojure is using <code>atoms</code>.</p>

<p>```clojure
(require &lsquo;[shadertone.core :as t])</p>

<p>(defonce cellular-growth (atom 0.0))</p>

<p>(t/start-fullscreen &ldquo;resources/shaders/electric.glsl&rdquo; :user-data {&ldquo;iCellularGrowth&rdquo; cellular-growth})</p>

<p>(swap! cellular-growth + 0.01)
```</p>

<h4>Hardware: Monome: <a href="http://monome.org">http://monome.org</a></h4>

<p>Something you don&rsquo;t see in the video is that I&rsquo;m using a 8x16 <a href="http://monome.org">Monome</a>. For this performance its primary function was a visual aid to show the beat/measure information.</p>

<p><img src="/images/monome.jpg" alt="Monome" /></p>

<p>The hardware is driven by Clojure communicating with the Monome through a serial port: <a href="https://github.com/josephwilk/monome-serial/tree/protocols">https://github.com/josephwilk/monome-serial/tree/protocols</a></p>

<h1>Live Coding</h1>

<p>Live coding music and graphics combines skills in sound engineering, 3d graphics, geometry, physics, musical theory, composition, improvisation &amp; hardware to name a few.</p>

<p>It is difficult, and requires a lot of work and practice.</p>

<p>But of all the code I&rsquo;ve written over the years this is one of the things I&rsquo;m most proud of. And i&rsquo;m only at the beginning of discovering what&rsquo;s possible.</p>
]]></content>
  </entry>
  
</feed>
