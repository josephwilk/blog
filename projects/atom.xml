<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: projects | Joseph Wilk]]></title>
  <link href="http://josephwilk.github.io//projects/atom.xml" rel="self"/>
  <link href="http://josephwilk.github.io/"/>
  <updated>2015-10-02T14:34:17+01:00</updated>
  <id>http://josephwilk.github.io/</id>
  <author>
    <name><![CDATA[Joseph Wilk]]></name>
    <email><![CDATA[joe@josephwilk.net]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[JavaScript Acting as a Robotic Agent ]]></title>
    <link href="http://josephwilk.github.io/projects/javascript-acting-as-a-robotic-agent.html"/>
    <updated>2008-02-23T17:06:18+00:00</updated>
    <id>http://josephwilk.github.io/projects/javascript-acting-as-a-robotic-agent</id>
    <content type="html"><![CDATA[<p>We can think of JavaScript running within a clients browser as a robotic agent. It has an environment in which it can sense things. The ability to look at the environment and make decisions based on plans.</p>

<p><img src="/images/blog/2007/12/clientagent.JPG" alt="clientagent.JPG" /></p>

<p>So whys that useful, well why is a robot useful? You can produce many different complex plans and give them to the robot and forget about it while it does the work potentially over and over again. If we are really lucky the robot can demonstrate some intelligence and deal with uncertainty.</p>

<p>Well I tried out a small part of this idea to build a server side service which delivered plans in JavaScript to the client. The JavaScript planning agent followed the plans. Its not a  intelligent robot but this is just a prototype. The plans where focused on validation conditions that a user needed to get through to post a form.</p>

<p>Example:</p>

<p><code>javascript
planCollection = Array();  
planCollection[0][0]= Array();
planCollection[0][0]= document.forms['iwfmsForm'].elements['age'].value&gt;18;
planCollection[0][1]= document.forms['iwfmsForm'].elements['ward'].value=='adult’;
</code></p>

<p>Timings where implied by ordering of plans in an array. i.e. for plan 0 &ndash;  value > 18 must happen before ward == adult.</p>

<p>Array dimensions imply different plans or forks in the plan.</p>

<p>Its a rather crude plan referencing form names and lots of low level javascript  but it helps simplify the boring bits!</p>

<h3>The Environment</h3>

<p>The environment represents the different states the form elements and the user can be in.</p>

<h3>The See process</h3>

<p>This function maps the form elements to their associated values.</p>

<h3>The Message Store</h3>

<p>The message store is used to represent events generated by the user. Events could be the result of clicking on a form element or clicking away from a form element. Such events generate messages which the plan executor responds to. These events are the points where the planner is activated.</p>

<h3>The Plans</h3>

<p>The plans represent a sequence of constraints within the JavaScript syntax on form elements. This type of action is not executed but used to help decided upon which plan path to follow.</p>

<h3>The Action process</h3>

<p>This process deals with selecting actions based on the see process which provides information about the form element’s values. There may only be a single plan and thus no decision to make. If there are multiple plans the decision to follow an action is only made for the first action of a plan. Which initial action to decide upon is decided by testing each of the first actions of the plans until one holds from the information provided by the see process. This plan is then used to further test the next constraint of the plan and so on. If multiple first actions hold each of the plans is tested to see if any of them reach the goal.</p>

<h3>The Goals</h3>

<p>The goals of the plans can be regarded as ensuring that all the conditions in the plan hold based on the data in the HTML form.</p>

<h3>The Execution of Actions</h3>

<p>Within the planner the actions are not executed. Since we are constraining the user all we need to do is find a valid plan path. The planner affects the environment by alerting the user if it cannot find any valid plan path.</p>

<p>The plan executor needs to be able to handle two different situations:</p>

<ul>
<li><p>Form submission
When the user inputs information into a form and then submits the form the planner will have to ensure that a valid plan path is reflected by the values that are held in the form. The form submission should be prevented if such a plan cannot be found.</p></li>
<li><p>Real time user input</p>

<ul>
<li><ol type="a">
<li><p>Guidance through a plan
As the user inputs information the planner needs to be aware of the current position in multiple plans and what effect this has on the current form element being filled in. Alerting the user if they have failed to achieve any plan as a result of the data they entered into the form.</p></li>
<li><ol type="a">
<li>Temporal ordering restrictions
The planner needs to be able to handle temporal restrictions on the form elements. Indicating in which order form elements have to be completed. Therefore we have to look at the impact that a real time input action will have on the rest of the form. For example one form element being completed may have the effect that another disabled form element can be activated for input.</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
</ul>


<h3>JavaScript Planning agent Code</h3>

<p><a href="https://github.com/josephwilk/iwfms/blob/master/javaScript/javaScriptPlanner.js">https://github.com/josephwilk/iwfms/blob/master/javaScript/javaScriptPlanner.js</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Latent Semantic Analysis in Python]]></title>
    <link href="http://josephwilk.github.io/projects/latent-semantic-analysis-in-python.html"/>
    <updated>2007-12-19T11:14:42+00:00</updated>
    <id>http://josephwilk.github.io/projects/latent-semantic-analysis-in-python</id>
    <content type="html"><![CDATA[<p>Latent Semantic Analysis (LSA) is a mathematical method that tries to bring out latent relationships within a collection of documents. Rather than looking at each document isolated from the others it looks at all the documents as a whole and the terms within them to identify relationships.</p>

<p>An example of LSA:
Using a search engine search for &ldquo;<em>sand</em>&rdquo;.</p>

<p>Documents are returned which do not contain the search term &ldquo;sand&rdquo; but contains terms like &ldquo;beach&rdquo;.</p>

<p>LSA has identified a latent relationship, &ldquo;<em>sand</em>&rdquo; is semantically close to &ldquo;beach&rdquo;.</p>

<p>There are some very good papers which describing LSA in detail:</p>

<ul>
<li><p>An introduction to LSA: <a href="http://lsa.colorado.edu/papers/dp1.LSAintro.pdf">http://lsa.colorado.edu/papers/dp1.LSAintro.pdf</a></p></li>
<li><p>Creating your own LSA space: <a href="http://www.andrew.cmu.edu/user/jquesada/pdf/bookSpacesRev1.pdf">http://www.andrew.cmu.edu/user/jquesada/pdf/bookSpacesRev1.pdf</a></p></li>
<li><p>Latent Semantic analysis: <a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">http://en.wikipedia.org/wiki/Latent_semantic_indexing</a></p></li>
</ul>


<p>This is an implementation of LSA in Python (2.4+). Thanks to scipy its rather simple!</p>

<h3>1 Create the term-document matrix</h3>

<p>We use the previous work in <a href="/projects/building-a-vector-space-search-engine-in-python.html">Vector Space Search</a> to build this matrix.</p>

<h3>2 tf-idf Transform</h3>

<p>Apply the tf-idf transform to the term-document matrix. This generally tends to help improve results with LSA.</p>

<p>{% codeblock lang:python %}
def tfidfTransform(self,):</p>

<pre><code>    """ Apply TermFrequency(tf)*inverseDocumentFrequency(idf) for each matrix element.
        This evaluates how important a word is to a document in a corpus

        With a document-term matrix: matrix[x][y]
            tf[x][y] = frequency of term y in document x / frequency of all terms in document x
            idf[x][y] = log( abs(total number of documents in corpus) / abs(number of documents with term y)  )
        Note: This is not the only way to calculate tf*idf
    """

    documentTotal = len(self.matrix)
    rows,cols = self.matrix.shape

    for row in xrange(0, rows): #For each document

            wordTotal= reduce(lambda x, y: x+y, self.matrix[row] )

            for col in xrange(0,cols): #For each term

                    #For consistency ensure all self.matrix values are floats
                    self.matrix[row][col] = float(self.matrix[row][col])

                    if self.matrix[row][col]!=0:

                            termDocumentOccurences = self.__getTermDocumentOccurences(col)

                            termFrequency = self.matrix[row][col] / float(wordTotal)
                            inverseDocumentFrequency = log(abs(documentTotal / float(termDocumentOccurences)))
                            self.matrix[row][col]=termFrequency*inverseDocumentFrequency
</code></pre>

<p>{% endcodeblock %}</p>

<h3>3 Singular Value Decomposition</h3>

<p>SVD: <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">http://en.wikipedia.org/wiki/Singular_value_decomposition</a></p>

<p>Determine U, Sigma, VT from our MATRIX from previous steps.</p>

<pre><code> U . SIGMA . VT = MATRIX
</code></pre>

<p>We use the <a href="http://www.scipy.org/">scipy</a> svd implementation here. Note that numpy (version: <strong>1.0.4 </strong>) also has an implementation of svd but I had lots of problems with it. I found it  did not work with anything larger than a 2 dimensional matrix.</p>

<p>{% codeblock lang:python %}</p>

<h1>Sigma comes out as a list rather than a matrix</h1>

<p> u,sigma,vt = linalg.svd(self.matrix)
 {% endcodeblock %}</p>

<h3>4 Reduce the dimensions of Sigma</h3>

<p>We generally delete the smallest coefficients in the diagonal matrix Sigma to produce Sigma'. The reduction of the dimensions of Sigma combines some dimensions such that they are on more than one term. The number of coefficients deleted can depend of the corpus used. It should be large enough to fit the real structure in the data, but small enough such that noise or unimportant details are not modelled.</p>

<p>The <strong>real difficulty and weakness of  LSA</strong> is knowing <strong>how many dimensions to remove</strong>. There is no exact method of finding the right dimensions. Generally <a href="http://mathworld.wolfram.com/L2-Norm.html">L2-norm</a> or <a href="http://en.wikipedia.org/wiki/Frobenius_norm#Frobenius_norm">Frobenius norm</a> are used.</p>

<h3>5 Calculate the Product with New Sigma'</h3>

<p>Finally we calculate:</p>

<pre><code>U . SIGMA' . VT = MATRIX'
</code></pre>

<p>{% codeblock lang:python %}</p>

<h1>Reconstruct MATRIX'</h1>

<p>reconstructedMatrix= dot(dot(u,linalg.diagsvd(sigma,len(self.matrix),len(vt))),vt)
{% endcodeblock %}</p>

<p>Giving use our final LSA matrix. We can now apply the same functionality used in Vector space search: searching and finding related scores for documents.</p>

<h2>LSA In Action &ndash; Matrices</h2>

<p>We start with out Model-Term frequency matrix with is generated from creating a <a href="/projects/building-a-vector-space-search-engine-in-python.html">Vector Space Search</a> with four documents (D1-D4):
D1:&ldquo;<em>The cat in the hat disabled</em>&rdquo;
D2:&ldquo;<em>A cat is a fine pet ponies.</em>&rdquo;
D3:&ldquo;<em>Dogs and cats make good pets</em>&rdquo;
D4:&ldquo;<em>I haven&rsquo;t got a hat.</em>&rdquo;</p>

<pre><code>    &lt;strong&gt;good  pet   hat   make  dog   cat   poni  fine  disabl&lt;/strong&gt;
D1 [+0.00 +0.00 +1.00 +0.00 +0.00 +1.00 +0.00 +0.00 +1.00 ]
D2 [+0.00 +1.00 +0.00 +0.00 +0.00 +1.00 +1.00 +1.00 +0.00 ]
D3 [+1.00 +1.00 +0.00 +1.00 +1.00 +1.00 +0.00 +0.00 +0.00 ]
D4 [+0.00 +0.00 +1.00 +0.00 +0.00 +0.00 +0.00 +0.00 +0.00 ]
</code></pre>

<p>Apply tf-idf transform:</p>

<pre><code>D1 [+0.00 +0.00 +0.23 +0.00 +0.00 +0.10 +0.00 +0.00 +0.46 ]
D2 [+0.00 +0.17 +0.00 +0.00 +0.00 +0.07 +0.35 +0.35 +0.00 ]
D3 [+0.28 +0.14 +0.00 +0.28 +0.28 +0.06 +0.00 +0.00 +0.00 ]
D4 [+0.00 +0.00 +0.69 +0.00 +0.00 +0.00 +0.00 +0.00 &lt;span style="color: #ff0000;"&gt;&lt;strong&gt;+0.00&lt;/strong&gt;&lt;/span&gt; ]
</code></pre>

<p>Perform SVD &ndash; Reduce Sigmas dimensions(removing the 2 smallest coefficients)</p>

<pre><code>D1 [+0.01 +0.01 +0.34 +0.01 +0.01 +0.03 +0.02 +0.02 +0.11 ]
D2 [-0.00 +0.17 -0.01 -0.00 -0.00 +0.08 +0.35 +0.35 +0.02 ]
D3 [+0.28 +0.14 -0.01 +0.28 +0.28 +0.06 -0.00 -0.00 +0.02 ]
D4 [-0.01 -0.01 +0.63 -0.01 -0.01 +0.04 -0.01 -0.01 &lt;strong&gt;&lt;span style="color: #ff0000;"&gt;+0.19&lt;/span&gt;&lt;/strong&gt; ]
</code></pre>

<p>Note the Word &lsquo;disabl&rsquo; despite not being in D4 now has a weighting in that document.</p>

<h3>Dependencies</h3>

<p><a href="http://www.scipy.org/">http://www.scipy.org/</a></p>

<h3>Problems</h3>

<p>LSA assumes the <a href="http://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a> where the <a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> has actually been observed.</p>

<h3>Source Code</h3>

<p>Available at github:</p>

<pre><code>git clone git://github.com/josephwilk/semanticpy.git
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a Vector Space Search Engine in Python]]></title>
    <link href="http://josephwilk.github.io/projects/building-a-vector-space-search-engine-in-python.html"/>
    <updated>2007-11-27T08:08:49+00:00</updated>
    <id>http://josephwilk.github.io/projects/building-a-vector-space-search-engine-in-python</id>
    <content type="html"><![CDATA[<p>A vector space search involves converting documents into vectors. Each dimension within the vectors represents a term. If a document contains that term then the value within the vector is greater than zero.</p>

<p>Here is an implementation of Vector space searching using python (2.4+).</p>

<h3>1 Stemming &amp; Stop words</h3>

<p>Fetch all terms within documents and clean &ndash; use a stemmer to reduce. A stemmer takes words and tries to reduce them to there base or root. Words which have a common stem often have similar meanings.
Example:
<em>CONNECTED
CONNECTING
CONNECTION
CONNECTIONS</em></p>

<p>all map to <em>CONNECT</em></p>

<p>We also remove any stopwords from the documents. [a,am,an,also,any,and] are all examples of stopwords in English.  Stop words have little value in search so we strip them. The stoplist used was taken from: <a href="ftp://ftp.cs.cornell.edu/pub/smart/english.stop">ftp://ftp.cs.cornell.edu/pub/smart/english.stop</a></p>

<pre><code> self.stemmer = PorterStemmer()
</code></pre>

<p>{% codeblock lang:python %}</p>

<pre><code> def removeStopWords(self,list):
         """ Remove common words which have no search value """
         return [word for word in list if word not in self.stopwords ]


 def tokenise(self, string):
         """ break string up into tokens and stem words """
         string = self.clean(string)
         words = string.split(" ")

         return [self.stemmer.stem(word,0,len(word)-1) for word in words]
</code></pre>

<p>{% endcodeblock %}</p>

<h3>2 Map Keywords to Vector Dimensions</h3>

<p>Map the vector dimensions to keywords using a dictionary: keyword=>position</p>

<p>{% codeblock lang:python %}
def getVectorKeywordIndex(self, documentList):</p>

<pre><code>    """ create the keyword associated to the position of the elements within the document vectors """

    #Mapped documents into a single word string
    vocabularyString = " ".join(documentList)

    vocabularyList = self.parser.tokenise(vocabularyString)
    #Remove common words which have no search value
    vocabularyList = self.parser.removeStopWords(vocabularyList)
    uniqueVocabularyList = util.removeDuplicates(vocabularyList)

    vectorIndex={}
    offset=0
    #Associate a position with the keywords which maps to the dimension on the vector used to represent this word
    for word in uniqueVocabularyList:
            vectorIndex[word]=offset
            offset+=1
    return vectorIndex  #(keyword:position)
</code></pre>

<p>{% endcodeblock %}</p>

<h3>3 Map Document Strings to Vectors.</h3>

<p>We use the simple Term Count Model. A more accurate model would be to use <a href="http://en.wikipedia.org/wiki/Tf-idf">tf-idf</a> (termFrequency-inverseDocumentFrequnecy).</p>

<p>{% codeblock lang:python %}
def makeVector(self, wordString):</p>

<pre><code>    """ @pre: unique(vectorIndex) """

    #Initialise vector with 0's
    vector = [0] * len(self.vectorKeywordIndex)
    wordList = self.parser.tokenise(wordString)
    wordList = self.parser.removeStopWords(wordList)
    for word in wordList:
            vector[self.vectorKeywordIndex[word]] += 1; #Use simple Term Count Model
    return vector
</code></pre>

<p>{% endcodeblock %}</p>

<h3>4 Find Related Documents</h3>

<p>We now have the ability to find related documents. We can test if two documents are in the concept space by looking at the the cosine of the angle between the document vectors.  We use the cosine of the angle as a metric for comparison. If the cosine is 1 then the angle is 0° and hence the vectors are parallel (and the document terms are related).  If the cosine is 0 then the angle is 90° and the vectors are perpendicular (and the document terms are not related).</p>

<p>We calculate the cosine between the document vectors in python using scipy.</p>

<p>{% codeblock lang:python %}
def cosine(vector1, vector2):</p>

<pre><code>    """ related documents j and q are in the concept space by comparing the vectors :
            cosine  = ( V1 * V2 ) / ||V1|| x ||V2|| """
    return float(dot(vector1,vector2) / (norm(vector1) * norm(vector2)))
</code></pre>

<p>{% endcodeblock %}</p>

<h3>5 Search the Vector Space!</h3>

<p>In order to perform a search across keywords we need to map the keywords to the vector space. We create a temporary document which represents the search terms and then we compare it against the document vectors using the same cosine measurement mentioned for relatedness.</p>

<p>{% codeblock lang:python %}
def search(self,searchList):</p>

<pre><code>    """ search for documents that match based on a list of terms """
    queryVector = self.buildQueryVector(searchList)

    ratings = [util.cosine(queryVector, documentVector) for documentVector in self.documentVectors]
    ratings.sort(reverse=True)
    return ratings
</code></pre>

<p>{% endcodeblock %}</p>

<h3>Further Extensions</h3>

<ol>
<li><p> Use tf-idf rather than the Term count model for term weightings</p></li>
<li><p> Instead of linear processing of all document vectors when searching for related content use:  <a href="http://en.wikipedia.org/wiki/Lanczos_method">Lanczos methods</a> OR a <a href="http://en.wikipedia.org/wiki/Neural_network">neural network</a>-like approach.</p></li>
<li><p> Moving towards <a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent Semantic analysis</a>, <a href="http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">Probabilistic latent semantic analysis</a> or <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet allocation</a>.</p></li>
</ol>


<h3>Third Party tools</h3>

<p>The stemmer used comes from:
<a href="http://tartarus.org/~martin/PorterStemmer/python.txt">http://tartarus.org/~martin/PorterStemmer/python.txt</a></p>

<p>And the library for performing cosine calculations comes from NumPy:
<a href="http://www.scipy.org/">http://www.scipy.org/</a></p>

<h3>Source</h3>

<p><a href="https://github.com/josephwilk/semanticpy.git">https://github.com/josephwilk/semanticpy.git</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prolog ASLDICN Event Calculus Planner]]></title>
    <link href="http://josephwilk.github.io/projects/prolog-asldicn-event-calculus-planner.html"/>
    <updated>2007-11-23T07:31:27+00:00</updated>
    <id>http://josephwilk.github.io/projects/prolog-asldicn-event-calculus-planner</id>
    <content type="html"><![CDATA[<p>The event calculus planner used within my thesis was based on Dr. Murray Shanahan&rsquo;s ASLDICN (Abductive SLD with Integrity constraints and proof by Negation) planner with compound action support. This planner is an adaptation from one published in one of Dr. Shanahan&rsquo;s research papers</p>

<p><a href="http://casbah.ee.ic.ac.uk/~mpsha/planners.html">http://casbah.ee.ic.ac.uk/%7Empsha/planners.html</a></p>

<p>The original planner only supports the generation of a single plan. I needed to support conditional planning. I wanted the planner to generate multiple plans representing the different ways of reaching the goal. The problem was how to convert the planner to generate all possible plans. Importantly ensuring that this does not cause infinite looping and no redundant plan solutions are generated.</p>

<p>My version of the planner add the following features:</p>

<ul>
<li><p>Conditional Planning</p></li>
<li><p>Impossible Predicate</p></li>
<li><p>Occured And NotOccured predicates</p></li>
</ul>


<h3>Prolog Event Calculus Planner</h3>

<p><a href="/workspace/prolog/eventCalculusPlanner.pl">Download eventCalculusPlanner.pl</a></p>

<p>[viewcode] src=../projects/prolog/eventCalculusPlanner.pl geshi=fortran[/viewcode]</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Prolog as CGI]]></title>
    <link href="http://josephwilk.github.io/projects/running-prolog-as-cgi.html"/>
    <updated>2007-11-23T07:27:25+00:00</updated>
    <id>http://josephwilk.github.io/projects/running-prolog-as-cgi</id>
    <content type="html"><![CDATA[<p>Prolog can be run as CGI by using a PHP wrapper script which invokes the Prolog engine from within PHP. Prolog can be invoked indicating Prolog files to load and goals to initially achieve once loaded.</p>

<p>Executing the following in PHP can spawn a process which runs Prolog.</p>

<pre><code>$cgiOutput = `sicstus --goal $goal. -l "$cgiPrologScriptToLoad"`;
</code></pre>

<p>This specific example is for Sicstus but most Prolog command lines have a similar format. Another possiblity is to setup Prolog as CGI, since any langauge can be CGI. I was running my code on a windows box and found it impossible for Prolog to direct the content to the command line and capture it for returning. If you&rsquo;re going the unix route you may want to look at <a href="http://clip.dia.fi.upm.es/Software/pillow/pillow.html">PiLLoWs</a> guide.</p>

<p>For form postings you can catch the post in PHP or a scripting language and create a prolog formated file which is passed to the prolog script when invoked.</p>

<p>You may want to have Prolog maintain state. This can be achieved through using a database. The database that I have used is <a href="http://www.sleepycat.com/">Berkeley DB</a> which SICStus has built in support for.</p>
]]></content>
  </entry>
  
</feed>
